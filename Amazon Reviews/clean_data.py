import pandas as pd
import re
import nltk
import os
import swifter
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer
from nltk.tokenize import TweetTokenizer
from time import time

# T·∫£i d·ªØ li·ªáu c·∫ßn thi·∫øt t·ª´ NLTK (n·∫øu ch∆∞a c√≥)
## nltk.download('stopwords')
## nltk.download('wordnet')
## nltk.download('omw-1.4')
## nltk.download('punkt')

stop_words = set(stopwords.words('english'))
important_words = {"not", "no", "only", "over"}  # Gi·ªØ l·∫°i c√°c t·ª´ quan tr·ªçng
stop_words -= important_words  # Lo·∫°i b·ªè c√°c t·ª´ quan tr·ªçng kh·ªèi stopwords
lemmatizer = WordNetLemmatizer()
tokenizer = TweetTokenizer()

# Chu·∫©n h√≥a t·ª´ vi·∫øt t·∫Øt
contractions = {
    "can't": "cannot", "won't": "will not", "n't": " not", "'re": " are",
    "'s": " is", "'d": " would", "'ll": " will", "'t": " not", "'ve": " have", "'m": " am"
}
def expand_contractions(text):
    for key, value in contractions.items():
        text = text.replace(key, value)
    return text

# H√†m l√†m s·∫°ch d·ªØ li·ªáu
def clean_text(review):
    if pd.isna(review) or not isinstance(review, str):
        return ""
    review = review.lower()  # Chuy·ªÉn v·ªÅ ch·ªØ th∆∞·ªùng
    review = expand_contractions(review)  # Chu·∫©n h√≥a t·ª´ vi·∫øt t·∫Øt
    review = re.sub(r'http\S+|www\S+', '', review)  # X√≥a URL
    review = re.sub(r'\d+', '', review)  # X√≥a s·ªë
    review = re.sub(r'[^a-z\s]', '', review)  # Lo·∫°i b·ªè k√Ω t·ª± ƒë·∫∑c bi·ªát, ch·ªâ gi·ªØ l·∫°i ch·ªØ c√°i v√† kho·∫£ng tr·∫Øng
    tokens = tokenizer.tokenize(review)  # Tokenization
    tokens = [word for word in tokens if word not in stop_words]  # Lo·∫°i b·ªè stopwords
    tokens = [lemmatizer.lemmatize(word) for word in tokens]  # Lemmatization
    return ' '.join(tokens)

# Danh s√°ch c√°c file c·∫ßn x·ª≠ l√Ω
files = ["train.csv", "test.csv", "validation.csv"]

for file in files:
    if not os.path.exists(file):
        print(f"‚ùå Error: File '{file}' not found!")
        continue  # B·ªè qua file kh√¥ng t·ªìn t·∫°i

    start_time = time()
    print(f"üîÑ Processing '{file}'...")

    # ƒê·ªçc file
    df = pd.read_csv(file)

    # Ki·ªÉm tra n·∫øu c√≥ c·ªôt 'Review'
    if "reviews" not in df.columns:
        print(f"‚ö† Warning: 'reviews' column not found in '{file}', skipping...")
        continue

    # L√†m s·∫°ch d·ªØ li·ªáu
    df["reviews"] = df["reviews"].astype(str).swifter.apply(clean_text)

    # ƒê·ªïi t√™n file ƒë·∫ßu ra th√†nh d·∫°ng "train_clean.csv", "test_clean.csv", "validation_clean.csv"
    output_file = file.replace(".csv", "_clean.csv")
    df.to_csv(output_file, index=False)
    print(f"‚úÖ Done! File cleaned and saved to '{output_file}' in {time() - start_time:.2f} seconds.")
